{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 查看Pipeline支持的任务类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio           audio-classification\n",
      "image           depth-estimation\n",
      "image           image-classification\n",
      "image           image-feature-extraction\n",
      "image           image-to-image\n",
      "multimodal      automatic-speech-recognition\n",
      "multimodal      document-question-answering\n",
      "multimodal      feature-extraction\n",
      "multimodal      image-segmentation\n",
      "multimodal      image-to-text\n",
      "multimodal      mask-generation\n",
      "multimodal      object-detection\n",
      "multimodal      visual-question-answering\n",
      "multimodal      zero-shot-audio-classification\n",
      "multimodal      zero-shot-image-classification\n",
      "multimodal      zero-shot-object-detection\n",
      "text            conversational\n",
      "text            fill-mask\n",
      "text            question-answering\n",
      "text            summarization\n",
      "text            table-question-answering\n",
      "text            text-classification\n",
      "text            text-generation\n",
      "text            text-to-audio\n",
      "text            text2text-generation\n",
      "text            token-classification\n",
      "text            translation\n",
      "text            zero-shot-classification\n",
      "video           video-classification\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines import SUPPORTED_TASKS\n",
    "\n",
    "tasks = []\n",
    "for k, v in SUPPORTED_TASKS.items():\n",
    "    tasks.append(f\"{v['type']:15} {k}\")\n",
    "for t in sorted(tasks):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pipeline 的创建与使用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programmer\\Anaconda3\\envs\\transformers39\\lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 根据任务类型直接创建 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "d:\\Programmer\\Anaconda3\\envs\\transformers39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\model.safetensors\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at distilbert/distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\714eb0fa89d2f80546fda750413ed43d93601a13\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 根据任务类型直接创建 Pipeline，这时会使用默认的某个英文模型\n",
    "pipe = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998525381088257}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 pipeline 进行文本分类\n",
    "pipe('very good!')  # 直接传入原始字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998525381088257},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997695088386536}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(['very good!', 'very bad!'])   # 传入原始字符串列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.9998525381088257}\n",
      "{'label': 'NEGATIVE', 'score': 0.9997695088386536}\n"
     ]
    }
   ],
   "source": [
    "def list_to_generator(lst):  \n",
    "    for item in lst:  \n",
    "        yield item  \n",
    "sentence_generator = list_to_generator(['very good!', 'very bad!'])  \n",
    "\n",
    "for res in pipe(sentence_generator):\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 指定任务类型和模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at uer/roberta-base-finetuned-dianping-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 可以在 https://huggingface.co/models 找到\n",
    "pipe = pipeline(\"text-classification\", model=\"uer/roberta-base-finetuned-dianping-chinese\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative (stars 1, 2 and 3)', 'score': 0.9743660688400269}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"我觉得不太行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 先加载模型，再创建Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at uer/roberta-base-finetuned-dianping-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 这种方式必须同时指定 model 和 tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative (stars 1, 2 and 3)', 'score': 0.9743660688400269}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"我觉得不太行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 使用 GPU 进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "0.07967857599258422\n"
     ]
    }
   ],
   "source": [
    "# 查看模型的运行设备\n",
    "print(pipe.model.device)\n",
    "\n",
    "# 在 CPU 运行会很慢\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "times = []\n",
    "for i in range(100):\n",
    "    start = time.time()\n",
    "    pipe(\"我觉得不太行\")\n",
    "    torch.cuda.synchronize()  # 阻塞CPU线程，直到所有在当前设备上排队的CUDA核心完成执行为止\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "print(np.mean(times))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at uer/roberta-base-finetuned-dianping-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 通过 device 参数指定执行设备\n",
    "pipe = pipeline(\"text-classification\", model=\"uer/roberta-base-finetuned-dianping-chinese\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "0.022842261791229248\n"
     ]
    }
   ],
   "source": [
    "# 查看模型的运行设备\n",
    "print(pipe.model.device)\n",
    "\n",
    "# 在 GPU 运行会更快\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "times = []\n",
    "for i in range(100):\n",
    "    start = time.time()\n",
    "    pipe(\"我觉得不太行\")\n",
    "    torch.cuda.synchronize()  # 阻塞CPU线程，直到所有在当前设备上排队的CUDA核心完成执行为止\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "print(np.mean(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 确定 Pipeline 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-chinese-extractive-qa\\snapshots\\9b02143727b9c4655d18b43a69fc39d5eb3ddd53\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-chinese-extractive-qa\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-chinese-extractive-qa\\snapshots\\9b02143727b9c4655d18b43a69fc39d5eb3ddd53\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-chinese-extractive-qa\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-chinese-extractive-qa\\snapshots\\9b02143727b9c4655d18b43a69fc39d5eb3ddd53\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at uer/roberta-base-chinese-extractive-qa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-chinese-extractive-qa\\snapshots\\9b02143727b9c4655d18b43a69fc39d5eb3ddd53\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-chinese-extractive-qa\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-chinese-extractive-qa\\snapshots\\9b02143727b9c4655d18b43a69fc39d5eb3ddd53\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-chinese-extractive-qa\\snapshots\\9b02143727b9c4655d18b43a69fc39d5eb3ddd53\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-chinese-extractive-qa\\snapshots\\9b02143727b9c4655d18b43a69fc39d5eb3ddd53\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-chinese-extractive-qa\\snapshots\\9b02143727b9c4655d18b43a69fc39d5eb3ddd53\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-chinese-extractive-qa\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-chinese-extractive-qa\\snapshots\\9b02143727b9c4655d18b43a69fc39d5eb3ddd53\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-chinese-extractive-qa\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 先加载一个 qa 模型\n",
    "qa_pipe = pipeline(\"question-answering\", model=\"uer/roberta-base-chinese-extractive-qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.question_answering.QuestionAnsweringPipeline at 0x212b7c05490>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用 jupyter 查看这个 pipeline 的类型，属于QuestionAnsweringPipeline \n",
    "# 之后可以到 QuestionAnsweringPipeline 类的 __call__ 方法中查看它支持的输入格式。或者查文档也行\n",
    "qa_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.7320804595947266, 'start': 6, 'end': 8, 'answer': '北京'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pipe(question='中国的首都是哪里？', context=\"中国的首都是北京\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Pipeline 背后的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\wwxc9\\.cache\\huggingface\\hub\\models--uer--roberta-base-finetuned-dianping-chinese\\snapshots\\25faf1874b21e76db31ea9c396ccf2a0322e0071\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at uer/roberta-base-finetuned-dianping-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2769, 6230, 2533,  679, 1922, 6121,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7459, -1.8919]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor([[0.9744, 0.0256]], grad_fn=<SoftmaxBackward0>)\n",
      "negative (stars 1, 2 and 3)\n"
     ]
    }
   ],
   "source": [
    "# 1. 初始化 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "\n",
    "# 2. 初始化 model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "\n",
    "# 3. 输入预处理\n",
    "input_text = \"我觉得不太行\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "print(inputs)  # {'input_ids': tensor([[ 101, 2769, 6230, 2533,  679, 1922, 6121,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
    "\n",
    "# 4. 模型预测\n",
    "res = model(**inputs)\n",
    "print(res)     # SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7459, -1.8919]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
    "\n",
    "# 5. 结果后处理\n",
    "logits = res.logits\n",
    "logits = torch.softmax(logits, dim=-1)\n",
    "print(logits)  # tensor([[0.9744, 0.0256]], grad_fn=<SoftmaxBackward0>) 正面情感/负面情感\n",
    "pred = torch.argmax(logits).item()        # 0\n",
    "result = model.config.id2label.get(pred)  \n",
    "print(result)  # negative (stars 1, 2 and 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
